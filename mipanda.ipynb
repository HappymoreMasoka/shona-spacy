{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuEIGTIHVSDG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "shona_morph.py\n",
        "\n",
        "Shona morphological analyzer as a spaCy pipeline component.\n",
        "Uses a baseline/mkanganwi root dictionary + noun class (mipanda) prefixes.\n",
        "Annotates tokens with ._.shona (dict) containing:\n",
        "  - prefixes: list[str]\n",
        "  - root: str\n",
        "  - suffixes: list[str]\n",
        "  - lemma: str\n",
        "  - meaning: str\n",
        "  - noun_class_num: int or None\n",
        "  - noun_class_label: str or None\n",
        "\"\"\"\n",
        "\n",
        "from typing import List, Dict, Optional\n",
        "import spacy\n",
        "from spacy.tokens import Doc, Token, Span\n",
        "\n",
        "class ShonaAnalyzerSimple:\n",
        "    def __init__(self, mode: str = \"baseline\"):\n",
        "        self.mode = mode\n",
        "        if mode == \"baseline\":\n",
        "            self.INFLECTIONAL_PREFIXES = [\n",
        "                # common inflectional prefixes + many noun-class prefixes\n",
        "                \"ndi\",\"nd\",\"va\",\"v\",\"ha\",\"ta\",\"ma\",\"chi\",\"zvi\",\"ru\",\"ka\",\"tu\",\"hu\",\"ku\",\"pa\",\"mu\",\"ri\",\"sa\",\"se\",\"yo\",\n",
        "                # noun class single-token forms\n",
        "                \"mw\", \"mi\", \"m\", \"zv\", \"dz\", \"dzv\", \"sv\", \"zi\", \"kw\", \"tw\", \"ruw\"\n",
        "            ]\n",
        "            self.DERIVATIONAL_SUFFIXES = [\n",
        "                \"a\",\"i\",\"e\",\"o\",\"an\",\"ana\",\"sa\",\"tu\",\n",
        "                \"is\",\"ir\",\"er\",\"ur\",\"unur\",\"w\",\"iw\",\"irw\",\"ri\"\n",
        "            ]\n",
        "            self.ROOTS = {\n",
        "                \"famb\":\"walk\",\n",
        "                \"gar\":\"sit/stay\",\n",
        "                \"tuk\":\"scold\",\n",
        "                \"bik\":\"cook\",\n",
        "                \"sung\":\"tie\",\n",
        "                \"dy\":\"eat\",\n",
        "            }\n",
        "        elif mode == \"mkanganwi\":\n",
        "            self.INFLECTIONAL_PREFIXES = [\n",
        "                \"ha\",\"ndi\",\"va\",\"ti\",\"ri\",\"ku\",\"mu\",\"chi\",\"zvi\",\"ru\",\"ma\",\"pa\",\"sa\",\"se\",\"yo\",\n",
        "                \"mw\",\"mi\",\"m\",\"zv\",\"dz\",\"sv\",\"zi\",\"tw\"\n",
        "            ]\n",
        "            self.DERIVATIONAL_SUFFIXES = [\n",
        "                \"a\",\"i\",\"e\",\"o\",\"an\",\"ana\",\"sa\",\"tu\",\n",
        "                \"is\",\"ir\",\"er\",\"ur\",\"unur\",\"w\",\"iw\",\"irw\"\n",
        "            ]\n",
        "            self.ROOTS = {\n",
        "                \"tuk\":\"scold\",\n",
        "                \"famb\":\"walk\",\n",
        "                \"gar\":\"sit/stay\",\n",
        "                \"sung\":\"tie\",\n",
        "                \"bik\":\"cook\",\n",
        "                \"nzwa\":\"hear/feel\",\n",
        "                \"da\":\"love\",\n",
        "            }\n",
        "        else:\n",
        "            raise ValueError(\"Mode must be 'baseline' or 'mkanganwi'.\")\n",
        "\n",
        "    def strip_prefixes(self, word: str) -> (List[str], str):\n",
        "        \"\"\"\n",
        "        Greedy-first-match prefix stripping: collect at most one prefix by default.\n",
        "        We allow repeated stripping if multiple prefixes are plausible.\n",
        "        \"\"\"\n",
        "        prefixes = []\n",
        "        root = word\n",
        "        changed = True\n",
        "        # try a couple of iterations to catch stacked prefixes like 'va-mu-...' (rare)\n",
        "        for _ in range(3):\n",
        "            matched = False\n",
        "            # sort by length descending to prefer longer matches (e.g., 'zvi' before 'z')\n",
        "            for pref in sorted(self.INFLECTIONAL_PREFIXES, key=lambda s: -len(s)):\n",
        "                if root.startswith(pref) and len(root) > len(pref) + 1:\n",
        "                    prefixes.append(pref)\n",
        "                    root = root[len(pref):]\n",
        "                    matched = True\n",
        "                    break\n",
        "            if not matched:\n",
        "                break\n",
        "        return prefixes, root\n",
        "\n",
        "    def strip_suffixes(self, root: str) -> (List[str], str):\n",
        "        \"\"\"\n",
        "        Greedy-first-match suffix stripping.\n",
        "        \"\"\"\n",
        "        suffixes = []\n",
        "        cur = root\n",
        "        for _ in range(3):\n",
        "            matched = False\n",
        "            for suf in sorted(self.DERIVATIONAL_SUFFIXES, key=lambda s: -len(s)):\n",
        "                if cur.endswith(suf) and len(cur) > len(suf) + 1:\n",
        "                    suffixes.append(suf)\n",
        "                    cur = cur[:-len(suf)]\n",
        "                    matched = True\n",
        "                    break\n",
        "            if not matched:\n",
        "                break\n",
        "        return suffixes, cur\n",
        "\n",
        "    def analyze_word(self, word: str) -> Dict:\n",
        "        word = word.strip()\n",
        "        analysis = {\"word\": word, \"prefixes\": [], \"root\": word, \"suffixes\": [], \"lemma\": None, \"meaning\": \"unknown\"}\n",
        "        prefixes, rem = self.strip_prefixes(word)\n",
        "        analysis[\"prefixes\"] = prefixes\n",
        "        suffixes, root = self.strip_suffixes(rem)\n",
        "        analysis[\"suffixes\"] = suffixes\n",
        "        analysis[\"root\"] = root\n",
        "        analysis[\"lemma\"] = root\n",
        "        analysis[\"meaning\"] = self.ROOTS.get(root, \"unknown\")\n",
        "        return analysis\n",
        "\n",
        "\n",
        "# --- Noun-class (mipanda) data from your notes (concise mapping) ---\n",
        "# This maps common surface prefixes to noun class numbers/labels and a short description.\n",
        "NOUN_CLASS_PREFIXES = {\n",
        "    # class : [prefixes...]\n",
        "    1:  {\"prefixes\": [\"mu\",\"mw\"],    \"label\": \"Mupanda 1 (person singular)\"},\n",
        "    1.1:{\"prefixes\": [\"Ø\"],          \"label\": \"Mupanda 1a (proper nouns / null prefix)\"},\n",
        "    2:  {\"prefixes\": [\"va\",\"v\"],     \"label\": \"Mupanda 2 (people plural)\"},\n",
        "    2.1:{\"prefixes\": [\"va\",\"a\"],     \"label\": \"Mupanda 2a (Manyika a/va mismatch)\"},\n",
        "    2.2:{\"prefixes\": [\"a\"],          \"label\": \"Mupanda 2b (Zezuru a-)\"},\n",
        "    3:  {\"prefixes\": [\"m\",\"mw\"],     \"label\": \"Mupanda 3 (various things, some mass nouns)\"},\n",
        "    4:  {\"prefixes\": [\"mi\"],         \"label\": \"Mupanda 4 (plural of 3)\"},\n",
        "    5:  {\"prefixes\": [\"ri\",\"z\"],     \"label\": \"Mupanda 5 (some singulars like 'ziso')\"},\n",
        "    6:  {\"prefixes\": [\"ma\"],         \"label\": \"Mupanda 6 (plural of 5)\"},\n",
        "    7:  {\"prefixes\": [\"chi\",\"ch\"],   \"label\": \"Mupanda 7 (instrumental/augmentative etc.)\"},\n",
        "    8:  {\"prefixes\": [\"zvi\",\"zv\"],   \"label\": \"Mupanda 8 (plural of 7)\"},\n",
        "    9:  {\"prefixes\": [\"N\",\"n\",\"mb\",\"m\",\"h\"], \"label\": \"Mupanda 9 (class with nasal/zero prefix: imba, mbudzi, huni)\"},\n",
        "    10: {\"prefixes\": [\"dzi\",\"dz\"],   \"label\": \"Mupanda 10 (plural of 9)\"},\n",
        "    11: {\"prefixes\": [\"ru\",\"rw\"],    \"label\": \"Mupanda 11 (abstracts/collectives like 'ruoko')\"},\n",
        "    12: {\"prefixes\": [\"ka\"],         \"label\": \"Mupanda 12 (diminutive)\"},\n",
        "    13: {\"prefixes\": [\"tu\",\"tw\"],    \"label\": \"Mupanda 13 (plural diminutive)\"},\n",
        "    14: {\"prefixes\": [\"u\",\"hu\",\"hw\"],\"label\": \"Mupanda 14 (abstracts, states)\"},\n",
        "    15: {\"prefixes\": [\"ku\"],         \"label\": \"Mupanda 15 (infinitive nouns, verb nouns)\"},\n",
        "    16: {\"prefixes\": [\"pa\"],         \"label\": \"Mupanda 16 (locative 'at')\"},\n",
        "    17: {\"prefixes\": [\"ku\"],         \"label\": \"Mupanda 17 (locative 'to')\"},\n",
        "    17.1:{\"prefixes\":[\"Ø\"],          \"label\": \"Mupanda 17a (locative null prefix forms)\"},\n",
        "    18: {\"prefixes\": [\"mu\"],         \"label\": \"Mupanda 18 (in/on locative 'in')\"},\n",
        "    19: {\"prefixes\": [\"sv\",\"svi\"],   \"label\": \"Mupanda 19 (Karanga forms)\"},\n",
        "    21: {\"prefixes\": [\"zi\",\"z\"],     \"label\": \"Mupanda 21 (augmentative / growth)\"},\n",
        "}\n",
        "\n",
        "# Build a reverse lookup: prefix -> class number & label\n",
        "PREFIX_TO_CLASS = {}\n",
        "for num, info in NOUN_CLASS_PREFIXES.items():\n",
        "    for p in info[\"prefixes\"]:\n",
        "        # normalize to lowercase and canonical form\n",
        "        PREFIX_TO_CLASS[p.lower()] = (num, info[\"label\"])\n",
        "\n",
        "\n",
        "def detect_noun_class_from_prefix(prefixes: List[str]) -> (Optional[float], Optional[str]):\n",
        "    \"\"\"\n",
        "    Given a list of prefixes detected (in order), attempt to map the first/narrowest\n",
        "    prefix to a noun class. We check longest prefix first.\n",
        "    \"\"\"\n",
        "    if not prefixes:\n",
        "        return None, None\n",
        "    # try all prefixes in order: pick longest match\n",
        "    for pref in sorted(prefixes, key=lambda s: -len(s)):\n",
        "        key = pref.lower()\n",
        "        if key in PREFIX_TO_CLASS:\n",
        "            num, label = PREFIX_TO_CLASS[key]\n",
        "            return num, label\n",
        "        # try small-normalizations\n",
        "        if key.startswith(\"m\") and \"m\" in PREFIX_TO_CLASS:\n",
        "            return PREFIX_TO_CLASS[\"m\"]\n",
        "    return None, None\n",
        "\n",
        "\n",
        "# --- spaCy component ---\n",
        "class ShonaMorphComponent:\n",
        "    name = \"shona_morph\"\n",
        "\n",
        "    def __init__(self, nlp=None, mode: str = \"baseline\"):\n",
        "        self.nlp = nlp\n",
        "        self.analyzer = ShonaAnalyzerSimple(mode=mode)\n",
        "        # register token extension to hold the analysis dict\n",
        "        if not Token.has_extension(\"shona\"):\n",
        "            Token.set_extension(\"shona\", default=None)\n",
        "\n",
        "    def __call__(self, doc: Doc) -> Doc:\n",
        "        for token in doc:\n",
        "            txt = token.text\n",
        "            # analyze token form\n",
        "            analysis = self.analyzer.analyze_word(txt)\n",
        "            # detect noun class from prefixes if token appears nominal (simple heuristic)\n",
        "            noun_class_num, noun_class_label = detect_noun_class_from_prefix(analysis[\"prefixes\"])\n",
        "            analysis[\"noun_class_num\"] = noun_class_num\n",
        "            analysis[\"noun_class_label\"] = noun_class_label\n",
        "            # set extension\n",
        "            token._.shona = analysis\n",
        "        return doc\n",
        "\n",
        "\n",
        "# Convenience factory for spaCy pipeline registration (if you want to use @nlp.add_pipe)\n",
        "def make_shona_morph_component(nlp, name=\"shona_morph\", mode=\"baseline\"):\n",
        "    return ShonaMorphComponent(nlp=nlp, mode=mode)\n",
        "\n",
        "\n",
        "# --- Example usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    # pip install spacy\n",
        "    nlp = spacy.blank(\"en\")   # we only use tokenizer and token objects; language doesn't matter\n",
        "    # create component and add to pipeline\n",
        "    comp = ShonaMorphComponent(nlp=nlp, mode=\"baseline\")\n",
        "    nlp.add_pipe(comp, name=comp.name, last=True)\n",
        "\n",
        "    examples = [\n",
        "        \"murume\", \"vanhu\", \"mwana\", \"vietete\", \"mugoti\", \"moto\", \"miri\", \"ziso\", \"dzimba\", \"ruoko\",\n",
        "        \"kuzasi\", \"pachivanze\", \"mugoti\"\n",
        "    ]\n",
        "\n",
        "    for w in examples:\n",
        "        doc = nlp(w)\n",
        "        t = doc[0]\n",
        "        print(f\"\\nWORD: {w}\")\n",
        "        for k, v in t._.shona.items():\n",
        "            print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xgikRsyXZa-n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}